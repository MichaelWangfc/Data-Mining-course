{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# svm公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gbdt_formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ J_\\alpha(x) = \\sum_{m=0}^\\infty \\frac{(-1)^m}{m! \\Gamma (m + \\alpha + 1)} {\\left({ \\frac{x}{2} }\\right)}^{2m + \\alpha} \\text {，行内公式示例} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.加法模型<br>\n",
    "$ \\hat{y}= \\sum_{m=1}^K f_k(x) $\n",
    "\n",
    "2.前向分步算法（Forward stepwise）<br>\n",
    "$ f_m = f_{m-1}(x)+ h(x)  $"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.第 t-1 次迭代：$ f_{t-1}(x) $<br>\n",
    "损失函数： $ L(y,f_{t-1}(x))$\n",
    "\n",
    "4.第 t 次迭代：$f_{t}(x)$<br>\n",
    "损失函数： $𝐿(𝑦, 𝑓_𝑡 (𝑥))=𝐿(𝑦, 𝑓_{𝑡−1}(𝑥)+ℎ_𝑡(𝑥))$\n",
    "\n",
    "我们本轮迭代的目标是找到一个CART回归树模型的弱学习器$ℎ_𝑡(𝑥)$，让本轮的损失函数$𝐿(𝑦, 𝑓_𝑡 (𝑥))=𝐿(𝑦, 𝑓_{𝑡−1}(𝑥)+ℎ_𝑡(𝑥))$最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。\n",
    "\n",
    "$f_t(x) = arg min_{f_t(x)} L(y,f_t(x)) $\n",
    "\n",
    "我们希望构建一个沿着损失函数的负梯度方向降低的学习器,损失函数的负梯度方向:\n",
    "\n",
    "$r_{ti} = -[\\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)}]_{f(x)=f_{t-1}(x)}$  \n",
    "其中$f_t{(x)} = (f_t(x_1),f_t(x_2),...,f_t(x_N))$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "步骤1：第t轮的第i个样本的损失函数的负梯度表示为<br>\n",
    "$r_{ti} = -[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)}]_{f(x)=f_{t-1}(x)}$\n",
    "\n",
    "步骤2：针对每个样本的损失函数的负梯度$\\{𝑥_𝑖,𝑟_{𝑡𝑖}\\}_{𝑖=1}^𝑁$拟合一个CART回归树:  \n",
    "\\begin{align}\n",
    "${\\{R_j,b_j\\}}_i^{J_t} = arg min_{R_j,b_j}\\sum_{i=1}^N [r_{ti} - f_t(x_i;{\\{R_j,b_j\\}}_{1}^{J_t})]^2$\\\\\n",
    "\\end{align}\n",
    "步骤3：针对每个叶子节点 $𝑅_{𝑗𝑡}$，我们求出使损失函数最小，也就是拟合叶子节点最好的的输出值  \n",
    "\\begin{align}\n",
    "\\gamma_{jt} = arg min_{\\gamma} \\sum_{x_i\\in R_{jt}}L(y_i,f_{t-1}(x_i)+\\gamma)\\\\ \n",
    "\\end{align}\n",
    "得到弱学习器：  \n",
    "\\begin{align}\n",
    "h_t(x) = \\sum_{j=1}^{J_t}\\gamma_{jt}I(x\\in R_{jt})\\\\\n",
    "\\end{align}\n",
    "步骤4：更新强学习器：\n",
    "\\begin{align}\n",
    "f_{t}(x) = f_{t-1}(x)+\\nu \\sum_{j=1}^{J_t}\\gamma_{jt}I(x\\in R_{jt})\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入$\\{(x_i,y_i)\\}_{i=1}^N,M,L,...$  \n",
    "1.初始化:  \n",
    "$f_0(x) = arg min_r \\sum_{i=1}^N L(y_i,r)$\n",
    "\n",
    "2.迭代计算：    \n",
    "for t=1 to M do    \n",
    "2.1 $r_{ti} = -[\\frac{\\partial L(x,f(x_i))}{\\partial f(x_i)}]_{f(x)=f_{t-1}(x)}$   \n",
    "\n",
    "2.2 ${\\{R_j,b_j\\}}_i^{J_t} = arg min_{R_j,b_j}\\sum_{i=1}^N [r_{ti} - f_t(x_i;{\\{R_j,b_j\\}}_{1}^{J_t})]^2$   \n",
    "\n",
    "2.3 $\\gamma_{jt} = arg min_{\\gamma} \\sum_{x_i\\in R_{jt}}L(y_i,f_{t-1}(x_i)+\\gamma)$     \n",
    "\n",
    "$h_t(x) = \\sum_{j=1}^{J_t}\\gamma_{jt}I(x\\in R_{jt})$    \n",
    "\n",
    "2.4 $f_{t}(x) = f_{t-1}(x)+\\nu \\sum_{j=1}^{J_t}\\gamma_{jt}I(x\\in R_{jt}) $   \n",
    "end  \n",
    "输出 $F_M(x)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
